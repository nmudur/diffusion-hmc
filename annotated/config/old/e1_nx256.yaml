seed: 20
data:
  path: '../../data_processed/LogMaps_Mcdm_IllustrisTNG_LH_z=0.00_Nx256_train.npy'
  conditional: True
  labels_subset:
    - 0
    - 1
  labels: '../../data_processed/params_IllustrisTNG.txt'
  transforms: 'minmax+randfliprot'
  labels_normalize: True

architecture:
  model: explicitconditional
  dim_mults:
    - 1
    - 2
    - 4
    - 8
  init_conv: False
  use_cond_dim_for_block: True
  non_linear_conditional_embedding: True
  unet_dim: 64
  circular_conv: True

diffusion:
  beta_schedule: log_custom_schedule
  schedule_args: {}
  timesteps: 1000
  sampler_args:
    sampler_type: 'uniform'

train:
  epochs: 1000
  batch_size: 20
  optimizer: Adam #make sure to add optimizer reset here if you WANT to reset the optimizer's state AND you're reading in from a pretrained checkpoint
  scheduler: MultiStepLR
  scheduler_args:
    milestones: [120, 140, 160, 180]
    gamma: 0.5
  learning_rate: 2e-4
  loss_args:
    loss_version: standard
    recon_loss: False
    prior_loss: False
    loss_type: huber
    recon_weight: 1.0
    prior_weight: 1.0


resume_ckp: 'checkpoint_224000.pt'
resume_name: 'Run_4-26_16-42'
resume_id: 'b38475bw'
